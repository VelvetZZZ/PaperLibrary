Title,Venue,Year,Topic / Keywords,Category,Authors,Link,Status,My Takeaway,Rating
MGDebugger: Closing the Last Mile of Code Generation with Hierarchical Debugging,ICLR 2025 (OpenReview),2025,"hierarchical debugging, code generation, LLM agents",Method,Yuling Shi; Songsong Wang; Chengcheng Wan; Xiaodong Gu,https://openreview.net/forum?id=dwQIVcW1du,To Read,"提出 MGDebugger 框架，将代码按层次拆分，从底层语法到高层算法逐步定位并修复错误，大幅提高 LLM 代码生成的正确率。




",⭐⭐⭐⭐（结构清晰，适合学习 LLM 调试思路）
REDO: Execution‑Free Runtime Error Detection for Coding Agents,ICLR 2025 (OpenReview),2025,"static analysis, runtime error detection, coding agents",System,Shengwei Li et al.,https://openreview.net/forum?id=THkF3VWSNv,To Read,"Combines LLMs with static analysis to detect runtime errors without executing code.结合静态分析与语言模型，实现在不运行代码的情况下检测潜在运行时错误。
",
Improve Code Generation with Feedback,ICLR 2025 (OpenReview),2025,"self-debugging pipeline, execution feedback",Method,Z. Xu et al.,https://openreview.net/pdf?id=CscKx97jBi,To Read,"Simulates manual debugging: locate errors via intermediate variable inspection and fix iteratively.通过模拟人类调试思路，利用变量反馈和执行结果让模型迭代修复生成代码。
",
Revisit Self-Debugging with Self-Generated Tests for Code Generation,ACL 2025 (Long),2025,"self-debugging, self-generated tests, code generation, runtime feedback",Method,Xiancai Chen; Zhengwei Tao; Kechi Zhang; Changzhi Zhou; Xinyu Zhang; Wanli Gu; Yuanpeng He; Mengdi Zhang; Xunliang Cai; Haiyan Zhao; Zhi Jin,https://aclanthology.org/2025.acl-long.881/,To Read,"Study of letting LLMs generate tests and use execution feedback to self-debug generated code.探讨让大模型自己生成测试样例、再利用执行反馈来修正代码的方法，从而提升模型的自我调试能力。
",
Otter: Generating Tests from Issues to Validate SWE Patches,ICML 2025 (Poster),2025,"test generation, software engineering, TDD",Benchmark / Tool,,https://icml.cc/virtual/2025/poster/44767,To Read,"Generates tests directly from issue descriptions to validate future patches; supports TDD.从软件 issue 自动生成测试用例，用于验证补丁正确性，实现自动化测试驱动开发（TDD）。
",
Automated Benchmark Generation for Repository‑Level Program Repair,ICML 2025 (Poster),2025,"benchmark, repository‑level repair, testing",System,,https://icml.cc/virtual/2025/poster/43922,To Read,"Automates creating realistic repo‑level environments to test patch correctness with human tests.自动创建包含真实项目环境的代码修复基准，方便测试模型在真实仓库场景下的补丁能力。
",
Code-SPA: Style Preference Alignment to Large Language Models for Code Debugging,ACL 2025 (Findings),2025,"code debugging, style alignment, robustness",Method,Tao Wen et al.,https://aclanthology.org/2025.findings-acl.912/,To Read,"Aligns code style to formats LLMs prefer to improve debugging on noisy code.通过让代码风格更贴合 LLM 偏好，提升模型在噪声代码调试任务中的稳定性和正确率。
",
MLDebugging: Towards Benchmarking Code Debugging Across Multi‑Library Scenarios,ACL 2025 (Findings),2025,"benchmark, code debugging, multi-library",Benchmark,Jinyang Huang; Xiachong Feng; Qiguang Chen; Hanjie Zhao; Zihui Cheng; Jiesong Bai; Jingxuan Zhou; Min Li; Libo Qin,https://aclanthology.org/2025.findings-acl.305.pdf,To Read,"Creates a benchmarking setup focusing on debugging across diverse library contexts.构建多库环境下的代码调试基准，用于系统评估不同模型在跨库代码错误修复中的表现。
",
COAST: Enhancing the Code Debugging Ability of LLMs via ...,NAACL 2025 (Findings),2025,"code debugging, LLMs",Method,W. Yang et al.,https://aclanthology.org/2025.findings-naacl.139.pdf,To Read,"Method to enhance LLMs’ debugging capability (details in paper).提出一种增强 LLM 调试能力的新框架，让模型能更有效地识别和修复代码错误。
",
RepoST: Scalable Repository‑Level Coding Environment Construction with Sandbox Testing,ICML 2025 (Workshop: CODEML),2025,"sandbox testing, execution feedback, dataset construction",System / Dataset,,https://icml.cc/virtual/2025/48167,To Read,"Builds scalable repo‑level environments using sandbox tests to enable feedback-driven training.利用沙箱测试自动构建可执行的代码环境，支持生成训练数据并进行调试反馈。
",