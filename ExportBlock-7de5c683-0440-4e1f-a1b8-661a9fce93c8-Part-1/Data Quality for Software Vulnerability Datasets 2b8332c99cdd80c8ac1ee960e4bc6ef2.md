# Data Quality for Software Vulnerability Datasets

Authors: Roland Croft, M. Ali Babar, M. Mehdi Kholoosi
Venue: ICSE 2023
Year: 2023
Topic / Keywords: Software Vulnerability, Data Quality, Machine Learning, Deep Learning
Category: System / Dataset
Link: https://arxiv.org/abs/2301.05456
Status: Reading
My Takeaway: 核心发现：研究了四个SOTA漏洞数据集（Big-Vul, Devign, D2A, Juliet），发现它们普遍存在严重的质量问题。具体数据：真实世界数据集的标签错误率高达 20-71%，且存在 17-99% 的数据重复率。影响：这些数据质量问题（准确性、唯一性、一致性）会导致漏洞预测模型（SVP）的性能评估虚高或无法有效学习，强调了数据清洗的重要性。

Abstract:

**背景与现状(Context & Background)：**

The use of learning-based techniques to achieve automated software vulnerability detection has been of longstanding interest within the software security *domain*. These data-driven solutions are enabled by ***large software vulnerability datasets*** used for training and benchmarking. 基于学习的技术在实现自动化软件漏洞检测方面的应用，一直是软件安全*领域*长期关注的焦点。这些数据驱动的解决方案依赖于大型的软件漏洞数据集，以用于训练和基准测试。

- ***large software vulnerability datasets(大型软件漏洞数据集)***
    
    是指包含大量程序样本、代码片段或完整软件项目，并带有“是否存在安全漏洞”标签的集合。这类数据集通常用于训练、评估和基准测试自动化软件漏洞检测模型，尤其是基于机器学习或深度学习的技术。
    

作者首先交代“大环境”。现在大家都在用**基于学习的技术**（比如深度学习、大模型）来自动检测软件漏洞。而这些技术能跑起来，核心在于要有**大规模的漏洞数据集**用来训练和跑分（Benchmarking）。

**指出问题(The "However" / Problem Statement)：**

However, we observe that the quality of the data powering these solutions is currently ill-considered, hindering the reliability and value of produced outcomes. Whilst awareness of software vulnerability data preparation challenges is growing, there has been little investigation into the potential negative impacts of software vulnerability data quality. For instance, we lack confirmation that vulnerability labels are correct or consistent. 然而，我们观察到，目前驱动这些解决方案的数据质量并未受到足够重视，从而影响了结果的可靠性和价值。尽管人们对软件漏洞数据准备方面的挑战的关注日益增加，但对于软件漏洞数据质量可能带来的负面影响却缺乏深入研究。例如，我们无法确认漏洞标注是否正确或一致。

作者说：虽然大家都在搞模型，但**由于数据质量太差（ill-considered）**，导致结果不可靠。虽然有人意识到了，但很少有人**真正去调查**这些坏数据到底有多大的负面影响。Research Gap：大家都在卷模型结构，但忽略了数据，而且没人系统地研究过数据烂到什么程度，这就是作者要做的。

**本文的方法 (Methodology)：**

Our study seeks to address such shortcomings by inspecting ***five inherent data quality attributes*** for ***four state-of-the-art software vulnerability datasets*** and the subsequent impacts that issues can have on software vulnerability prediction models.

本研究旨在通过检查四个最先进的软件漏洞数据集中的五项固有数据质量属性，来弥补这一不足，并分析这些问题对软件漏洞预测模型可能产生的影响。

- **对象：** 找了4个目前最牛的（**State-of-the-art, SOTA**）数据集。
- **手段：** 检查了5个数据质量维度（比如标签对不对、有没有重复等）。
- **验证：** 看看这些烂数据对**预测模型**到底有多大影响。

**核心发现 (Key Findings / Results)：**

Surprisingly, we found that all the analyzed datasets exhibit some ***data quality problems.*** In particular, we found 20-71% of vulnerability labels to be **inaccurate** in real-world datasets, and 17-99% of data points were **duplicated**. 令人惊讶的是，我们发现所有被分析的数据集都存在一定的数据质量问题。具体而言，我们发现真实世界数据集中有20–71%的漏洞标签不准确，且17–99%的数据点存在重复。

- **标签错误率**高达 20%-71%（数据集里说这段代码有漏洞，其实可能没有，反之亦然。这对训练模型是灾难性的）。
- **重复率**高达 17%-99%。

**影响与结论 (Implications & Conclusion)：**

We observed that these issues could cause significant impacts on *downstream models*, either preventing effective model training or *inflating benchmark performance*. We advocate for the need to overcome such challenges. Our findings will enable better consideration and assessment of software vulnerability data quality in the future.我们观察到，这些问题可能会对下游模型产生显著影响，既可能阻碍有效的模型训练，也可能导致基准测试性能虚高。我们倡导应正视并克服此类挑战。我们的研究结果将有助于未来更好地评估和考量软件漏洞数据的质量。

这些问题会导致什么后果？

1. 模型根本训练不好（垃圾进，垃圾出）。
2. 或者**虚高了跑分成绩 (Inflating benchmark performance)**。这很致命，意味着以前很多论文宣称“我准确率99%”，其实可能是因为数据重复导致的虚假繁荣。

- **下游模型**(*downstream models):*
    
    **下游模型**是指依赖前序步骤（或前序模型）输出作为输入的模型。
    
    也就是说，数据先经过某些处理、特征提取或一个预训练模型，然后这些结果再被用来训练或运行后续的模型，这些后续的就是 **下游模型**。
    
    论文讨论 **漏洞数据质量**，指出数据问题会影响：
    
    - 模型训练效果
    - 模型性能评估
    - 实际部署的模型预测质量
    
    这些模型都属于 **下游模型**，即：
    
    > 基于这些数据集训练出来的漏洞预测模型
    > 
- **基准测试性能**(*benchmark performance):*
    
    Benchmark 是专门用于 **评估和比较模型性能的标准测试集**。
    
    它具有：
    
    - 固定的数据划分
    - 标准的评价指标
    - 广泛接受的对比基准
    
    **benchmark performance**就是模型在这些基准测试上跑出来的**成绩**。
    

评估现有的东西、发现现有基础的漏洞

### 1. Motivation (研究动机)

**1）数据驱动的依赖性**：基于深度学习的*软件漏洞预测（SVP）技术 *****高度依赖于*训练数据集*。

**核心观点：** AI 模型再强，也得靠数据喂出来，但软件漏洞的数据特别难搞。

1. **大背景：** 现在大家都在用深度学习（DL）来做软件漏洞预测（SVP），效果看起来比传统方法好 。
2. **硬约束：** 这些模型是Data-hungry的，必须要有大量标注好的代码（这就是“漏洞” vs “非漏洞”）才能训练 。
3. **核心痛点（The Pain Point）：**
    - 在软件安全领域，**没有“上帝视角” (No Oracle)** 。
    - *there is no oracle that can unfailingly prove the existence or absence of vulnerabilities from a codebase*
        
        不像图片分类，代码里有没有漏洞，有时候连专家都很难100%确定。
        

*Software Vulnerability Prediction (SVP) models use program analysis techniques to learn software vulnerability patterns automatically from historical examples. Due to the unstructured nature of source code, researchers have found the most success using Deep Learning (DL) techniques that learn from program syntax and semantics.软件漏洞预测（Software Vulnerability Prediction, SVP）模型利用程序分析技术，从历史实例中自动学习软件漏洞的模式。由于源代码具有非结构化的特性，研究者发现，最有效的方法是使用能够从程序语法和语义中学习的深度学习（Deep Learning, DL）技术。*

*Hence, SVP is a data-hungry process [16]: the models require a large training dataset of annotated code modules, labelled as vulnerable or non-vulnerable. However, acquiring a reliable vulnerability label source is a non-trivial task; there is no oracle that can unfailingly prove the existence or absence of vulnerabilities from a codebase [29]. Thus, researchers have relied on a variety of label sources to account for different shortcomings.(Page 2)因此，SVP 是一个对数据需求极高的过程 [16]：模型需要大量带注释的代码模块训练数据，这些模块被标注为“存在漏洞”或“无漏洞”。然而，获取可靠的漏洞标签来源并非易事；并不存在一个能够无误地证明某个代码库中漏洞存在或不存在的“预言机”[29]。因此，研究人员不得不依赖多种标签来源，以弥补各自存在的不足。*

**2）被忽视的数据质量**：目前的研究主要关注模型架构的改进，而忽视了支撑这些模型的数据质量。缺乏对***漏洞标签*** 正确性或一致性的验证。

以前的研究者想了**四种办法来收集数据：**

1. **安全供应商提供 (Security Vendor Provided)：**
    
    *来源：*比如 *NVD (National Vulnerability Database)*。
    
    - *逻辑：*官方通报了漏洞，我们就去抓对应的代码补丁。
    - *潜台词：*这是最权威的，但通常只有已知的、严重的漏洞。
        
        
        *Security Vendor Provided. Security vendors maintain vulnerability databases that aggregate information from various advisories. This provides a standardised collection of disclosed vulnerabilities. Examples include the National Vulnerability Database (NVD) [30], or the Snyk Vulnerability Database [31]. Vulnerability records often provide links to patches, which can then be traced to identify real-world source code and vulnerabilities.(Page 2)*
        
        - 1.什么是 Snyk 漏洞数据库
            
            **Snyk 漏洞数据库是一套由 Snyk 公司维护、面向开源软件生态的高质量漏洞记录库，提供详细、结构化、易追踪的漏洞信息，用于漏洞检测、安全审计和学术研究。**
            
        - 2.什么是patches(补丁)
            
            **Patch（补丁）** 是指对软件进行的 **修改代码或更新文件**，用于：
            
            - 修复安全漏洞
            - 修正常规功能缺陷（bugs）
            - 改进性能
            - 更新功能
            
            **Patch（补丁）= 用于修复漏洞或错误的代码修改，是软件更新中最基本的修复单位。**
            
        
         
        
2. **开发者提供 (Developer Provided)：**
    - *来源：* 比如 GitHub 的提交记录 (Commits) 。
    - *逻辑：* 只要程序员修了 Bug，我们就认为之前的代码有漏洞。
    - *缺陷：* 需要费力去挖历史记录，而且很多漏洞修复并没有明确标记 。
    
    *Developer Provided. Vulnerability databases may not
    properly document all vulnerabilities of a project [32].Hence, researchers may collect vulnerability fixing commits directly from developers via the development history or via a project’s issue tracking systems. However, this method requires additional effort to search development artefacts for security-related defects.*
    
3. **工具生成 (Tool Created)：**
    
    *来源：*用***静态分析工具***（比如 *Coverity、Fortify*）扫描代码。
    
    - *逻辑：*工具报错的地方就是漏洞。
    
    *缺陷：*工具本身就会误报（False Positives），用一个不准的工具去教 AI，AI 也会学歪。
    
    *Tool Created. Developer or security vendor-provided labels have a major limitation of only collecting reported vulnerabilities, which severely limits the number of examples that can be collected. (开发者或安全供应商提供的标签存在的主要限制)In reality, vulnerabilities can remain latent or undetected [33], which limits the dataset size and adds considerable label noise to the modules.To circumvent this, some researchers have utilised **static security analysis tools** to automatically produce labels for the source code [16], [34], [35]. This process relies heavily on the accuracy of the static analyser used, which
    is a source of contention.(Page 2)*
    
    - **静态分析工具（Static Analysis Tools）**
        
        **静态分析工具（Static Analysis Tools）** 是一种在 **不运行程序的情况下**，通过分析源代码或字节码来检测潜在问题的自动化工具。它广泛应用于软件质量检测、安全漏洞挖掘、合规性检查等领域。
        
        静态分析工具是通过“读代码”来发现问题的工具，它不需要程序实际运行。
        
        它会检查：
        
        - 代码结构
        - 控制流、数据流
        - 变量使用情况
        - 函数调用关系
        - 不安全的 API 使用
        - 可能导致漏洞的模式
        
        因此它特别适合发现安全缺陷和编码错误。   
        
        **1.Coverity**
        
        - 商业级超强静态分析工具
        - 被很多大公司（Google、微软）用来检查代码安全
        
        **2.Fortify**
        
        - 用于扫描安全风险
        - 在工业界非常流行
        
    
4. **合成数据 (Synthetically Created)：**
    
    *来源：*人工造假数据（比如著名的 Juliet 数据集)。
    
    - *逻辑：*我自己写一个肯定有漏洞的代码。
    
    *缺陷：***缺乏多样性**。真实世界的代码千奇百怪，这种"实验室代码"太干净、太简单了。
    
    *Synthetically Created. Finally, to bypass the limitations of other label sources, vulnerable code examples and annotations can be created artificially from known vulnerable patterns. Synthetically producing entries ensures label correctness at the cost of source code diversity [24].(Page 2)*
    

---

**3）“垃圾进，垃圾出” (Garbage In, Garbage Out)**：低质量的数据（如标签错误、数据重复）会导致模型学习到错误的模式，从而降低预测结果的可靠性和可信度。

- **现状：** 尽管大家都在拼命造数据集 ，但很少有人去检查这些**数据的质量 (Data Quality)** 。
    
    *However, we found that relatively little counterpart work has been conducted to understand the software vulnerability data quality. Whilst data quantity is important, an effective machine learning system requires adequate data quality [18].(Page 1)*
    
- **后果：** "Garbage in, garbage out"（垃圾进，垃圾出） 。
    - 如果数据有偏差，模型在**跑分 (Benchmark)** 时看起来很牛，一到**真实场景 (Real-world)** 就歇菜 。
    - 这解释了为什么工业界目前还不敢大规模用 AI 查漏洞
    
    *Data quality is an integral component of any data-driven system: garbage in, garbage out [19]. Certain data biases or misinformation can make benchmark performance results misleading [20]–[22]. This can cause models to fail to generalise to real-world scenarios [17], [23], [24], if they have not been trained with fair and realistic data.(Page 1)*
    
    *Consequently, the industry value and adoption of SVP models is uncertain [37].(Page 2)*
    
- **Gap（空白）：** 现有的研究要么只看某一小方面（比如标签准确性），要么不够系统 。
    
    *Existing studies that investigate data quality characteristics of software engineering datasets are currently non-systematic and limited.(Page 3)*
    
    *Similarly, Jimenez et al. [33] considered label accuracy within software vulnerability datasets. However, this approach fails to provide a complete picture. To make informed data decisions, there is a need for a systematized and objective investigation of data quality in the software engineering domain. (Page 3)*
    

**4）目标**：本研究旨在通过分析内在数据质量属性，揭示现有数据集的缺陷，并**量化**这些缺陷对模型性能的具体影响。

- **本文使命：** 基于 ***ISO/IEC 25012** 标准*（一个国际数据质量标准），**系统性地、定量地** 衡量这些数据集到底有多烂 。

- **ISO/IEC 25012 是什么？**
    
    SO/IEC 25012 是一个用来定义“数据质量是什么、由哪些方面组成”的国际标准框架。
    
    它告诉你：
    
    - 数据质量应该如何评价？
    - 有哪些不同维度？
    - 数据质量的关注点是什么？
    - 如何系统化地判断数据是否“好”？
    
    这就像是“国际公认的数据质量评价体系”。
    
    **⭐ ISO/IEC 25012 定义的数据质量 15 个特性:**
    
    - **内在数据质量（Intrinsic）**
        
        这些属性描述数据本身是否“正确可靠”：
        
        1. **准确性 Accuracy**
        2. **可信性 Credibility**
        3. **客观性 Objectivity**
        4. **声誉性 Reputation**
    - **系统依赖的数据质量（System-dependent）**
        
        跟数据的管理、存储方式有关：
        
        1. **可用性 Availability**
        2. **可检索性 Retrievability**
        3. **可访问性 Accessibility**
        4. **安全性 Security**
    - **用户依赖的数据质量（User-dependent）**
        
        跟数据是否“好用”有关：
        
        1. **相关性 Relevancy**
        2. **可理解性 Understandability**
        3. **操作性 Operability**
        4. **可读性 Readability**
    - **数据时间质量（Temporal quality）**
        1. **时效性 Timeliness**
        2. **持续性 Volatility**
    - **数据关系质量（Contextual relationship quality）**
        1. **一致性 Consistency**

### 2. Approach (研究方法)

- **评估框架**：基于 ISO/IEC 25012 数据质量标准，选取了 5 个核心**内在数据质量属性 (Inherent Data Quality Attributes)** 进行系统分析：

*For this study, we focused on purely **inherent data quality attributes**, as SVP models have not yet achieved widespread industrial application [37]. System-dependent attributes cannot be properly measured without an associated deployment context. In this sense, we focused on the data rather than how the data is used. Hence, our findings are not constrained to particular modelling techniques or features.(Page 3)*

*1）为什么不选择“系统依赖”的属性：*

- ISO/IEC 25012 把数据质量分为两类：**“内在的” (Inherent)** 和 **“系统依赖的” (System-dependent)** 。
- “系统依赖”的属性（比如数据读取速度快不快、系统崩了能不能恢复数据）取决于你用什么电脑、什么软件来跑这个数据。
- **关键理由：** 软件漏洞预测（SVP）技术目前还主要停留在学术研究阶段，还没有大规模的、标准化的“工业部署环境” 。
- 如果连统一的运行环境都没有，去测“系统依赖属性”是不公平的，也是没法测的。作者想关注的是**数据本身的质量**（inherent），而不是数据在某个特定系统上跑得怎么样的表现 。

2）ISO标准原有**准确性 Accuracy、可信性 Credibility、客观性 Objectivity、声誉性 Reputation** 中为什么作者把***可信性Credibility***扔了？

- **关键理由：** 作者认为这很难“量化” 。因为这些漏洞数据大都来自权威的 CVE 数据库、GitHub 官方提交记录或学术界的 SOTA 论文。作者**假设**这些来源本身是可信的，不需要去验证“GitHub 是不是一个骗子网站” 。
    - **Credibility（可信度）** 指的是我们对数据来源的信任程度（是不是假新闻、是不是伪造数据）。
    - **关键理由：** 作者认为这很难“量化” 。因为这些漏洞数据大都来自权威的 CVE 数据库、GitHub 官方提交记录或学术界的 SOTA 论文。作者**假设**这些来源本身是可信的，不需要去验证“GitHub 是不是一个骗子网站” 。

3）为什么引入***Uniqueness - 唯一性*** ？

**原文逻辑：**

- **Uniqueness（唯一性）** 对于普通软件工程可能没那么致命（数据库里有两行重复数据可能不影响程序运行）。
- **关键理由：** 但对于 **Machine Learning (ML)** 来说，重复数据是致命的！它会导致“数据泄露”（Data Leakage），让模型在考试时遇到做过的原题，导致分数虚高 。
- 因为这篇论文是针对 **AI/ML** 模型的，所以作者必须把“唯一性”作为一个独立且核心的指标提出来。

作者展现了批判性思维，没有死板照搬 ISO 标准。针对**机器学习（ML）这一特定场景，作者特别强调了 Uniqueness（唯一性）。虽然在传统软件工程中重复数据可能只是冗余，但在 AI 训练中，它会导致严重的数据泄露和评估虚高。这是结合具体应用场景（Context）** 做出的关键调整。

*总结：融55555555555t r55555555555555555555555t更好；‘/’‘*

1.  **减去环境依赖：** 因为 SVP 尚无工业标准环境，作者首先排除了 ISO 标准中所有**系统依赖型**指标（如可用性），只聚焦数据**内在**质量 。
2.  **减去主观难测：** 作者排除了 **Credibility（可信度）**，因为难以量化且默认权威来源可信，这是基于可行性的取舍 。
3.  **加上领域特需：** 针对机器学习易受‘数据泄露’影响的痛点，作者特意强调了 **Uniqueness（唯一性）** 的地位，这是对通用的 ISO 标准进行了**领域适配 (Domain Adaptation)** 。

*To identify inherent data quality attributes, we used the standardised data quality framework ISO/IEC 25012 [25]. This framework has been used for data quality assessment in both the Software Engineering [47] and Machine Learning
[18], [48] domains. ISO/IEC 25012 outlines five inherent data quality attributes: Accuracy, Consistency, Completeness, Currentness, and Credibility. We excluded the credibility attribute as it is difficult to quantify in existing software vulnerability datasets. Credibility indicates the level of trust that we have in a dataset; the authenticity of the data source or supplier. We need to ensure that our data points are free from contamination or fake information [36]. As datasets have been produced by peer-reviewed research or respected government organisations[10], we assume a level of trust in the data source and supplier of each dataset. Additionally, we considered a uniqueness data dimension, due to its prevalence in existing software engineering research [49], and its importance as highlighted by previous SVP researchers [24]. Table I summarises the selected inherent data quality attributes for analysis.此外，我们还考虑了**唯一性（uniqueness）**这一数据维度，因为它在现有的软件工程研究中具有普遍性 [49]，并且其重要性已被此前的 SVP 研究者强调 [24]。表 I 总结了本研究选取的用于分析的内在数据质量属性。(Page 3)*

![image.png](Data%20Quality%20for%20Software%20Vulnerability%20Datasets/image.png)

1. **Accuracy (准确性)**：标签是否正确反映了真实的漏洞状态。
    - **怎么测：** 这一点特别重要！因为没有“上帝视角”，作者采用了**人工审查 (Manual Analysis)**。
    - **操作细节：** 两位有2-5年安全经验的作者，随机抽取了70个样本，去读代码、读提交记录 (Commit message)，人工判断标签对不对 。
    - 过滤标准：**Functionally Relevant（功能相关性）**
        - ***什么是功能相关性（Functionally Relevant)?***
            
            根据漏洞报告（比如 CVE 描述）或工具报错日志，这段代码的**改动逻辑**是否与描述相符。
            
            作者通过人工阅读代码，判断代码中“发生变化的行”（Changed Lines） 是否真的执行了修复漏洞的功能逻辑。
            
            这是一个**替代指标 (Proxy Metric)**。最完美的数据集应该验证每个漏洞都能被 Exploit，但这不可能做到。所以作者退而求其次，只看“代码改动逻辑”。作为读者，我们要明白这还是隔了一层的。
            
            判断标签准确性，有没有标对。
            
        - *五步法：*
            
            ### 第一步：收集线索 (Evidence Collection)
            
            - **原文：** 提取漏洞相关的元数据。包括：
                - **Fixing Commit ID:** 修复漏洞的那次代码提交记录 。
                - **CVE-ID:** 漏洞编号（针对 Big-Vul 数据集）。
                - **Tool Trace:** 静态工具的扫描日志（针对 D2A 数据集）。
            
            ### 第二步：阅读案情报告 (Understanding the Crime)
            
            - **原文：** 阅读 Commit 的描述（程序员写的 Log）、NVD 的漏洞描述（官方通报）或工具的报错信息 。
            - **侦探逻辑：** 搞清楚这个“案子”到底是什么？
                - *例如：* 官方通报说是一个“缓冲区溢出（Buffer Overflow）”。那我就知道待会儿要重点看内存操作的代码。
            
            ### 第三步：现场勘查 (Scene Investigation) —— **最关键的一步**
            
            - **原文：** 检查**“被修改的行” (Changed lines)** 以及**整个函数的上下文** 。
            - **核心判断：** 基于对代码的理解，判断“这些修改”**是否真的修复了**“第二步里提到的那个漏洞”*。
            - **侦探逻辑（举例）：**
                - 如果案情是“修复缓冲区溢出”，而我看代码发现程序员确实加了一个 `if (len > max)` 的判断，那就是**Match（标签正确）**。
                - 如果案情是“修复缓冲区溢出”，但代码里只是把缩进调整了一下，或者改了个变量名（重构），跟内存安全毫无关系，那就是**Mismatch（标签错误/噪音）**。这就是为什么很多数据集标签不准的原因。
            
            ### 第四步：排除误判 (Alibi Check)
            
            - **原文：** 如果第三步觉得“这段代码看起来不像修漏洞的”，不要急着下定论。要回头去看**整个 Commit 的所有修改** 。
            - **侦探逻辑：** 为什么这个函数被标记了？
                - 可能真正的修复在另一个文件里（Root changes），而当前这个函数只是因为依赖关系被顺手改了一行（Collateral damage）。
                - 这就确认了当前这个函数是“无辜的”（即标签是错误的，它不是漏洞函数）。
            
            ### 第五步：陪审团合议 (Consensus)
            
            - **原文：** 两位作者独立做完上述步骤后，把意见不一致的样本拿出来讨论，达成共识 。
            - **科研逻辑：** 避免单人主观偏差（Subjective Bias）。在软件工程科研中，通常需要计算 **Cohen's Kappa** 系数来证明两个人的一致性很高，结果才可信。
    
    *Accuracy defines the correctness of the data points that comprise a dataset. This largely relates to the semantic label correctness; i.e., whether or not data points labelled as vulnerable or non-vulnerable genuinely align. It has previously been observed that non-vulnerable labels are unreliable in real-world datasets as there is no ground truth label source for this class [10], [13], [33]. No oracle can reliably ensure the security and absence of exploits in a given code snippet.(Page 4)*
    
    *Analysis. We determined if a label is correct via manual analysis with respect to each dataset's labelling mechanism: whether a vulnerability accurately represents the vulnerability report or static analysis tool warning that it was derived from. In this sense, we did not verify whether a vulnerability was actually exploitable, but rather whether a code snippet is functionally relevant to the reported vulnerability of each label. The following steps were taken to assess the label correctness of each entry:*
    
    *1）We first extracted information relating to the vulnerability and fixing commit of each dataset. All datasets provided a git fixing commit ID except Juliet. Big-Vul also provided CVE-IDs and D2A contained the static analysis tool trace.*
    
    *2）We read the fixing commit description and other available information (i.e., the vulnerability description from NVD for Big-Vul and the static tool trace for D2A) to gain an understanding of the vulnerability and the fixing commit changes.*
    
    *3）We then examined the changed lines in the fixing commit for the relevant function, as well as the entire function's code to understand the context of the changed lines.Based on this code comprehension, we made an assessment as to whether the changed lines were functionally relevant to the information from the previous step.*
    
    *4）If we did not interpret them as functionally relevant, we examined all the fixing commit changes to identify where the root changes were to understand why the flagged function was not relevant.*
    
    *5）Afterwards, the authors discussed the labels that were in disagreement and reached a consensus.*
    
    *To facilitate our manual review, we examined 70 random samples of each dataset (90% confidence level +/- 10% [54]). Two of the authors of this paper conducted this manual analysis independently; each of them had two to five years of software security-related experience gained in academia and industry. The two raters achieved a **Cohen Kappa value** of 0.627 [55], which implies moderate to strong agreement.(Page 5)*
    
    - **Cohen Kappa 是什么？**
        - **概念概述**
        
        **Cohen’s Kappa（科恩卡帕系数）** 是一种用于衡量
        
        👉 **两个标注者之间一致性（agreement）**
        
        👉 **并排除掉偶然一致（random agreement）**
        
        的统计指标。
        
        适用于分类任务的标注一致性评估（如“有漏洞/无漏洞”标签的质量分析）。
        
        - **🎯 为什么不用“准确率”来衡量一致性？**
        
        单纯的“表面一致率（Observed Agreement）”存在问题：
        
        1.类别不平衡会导致假一致
        
        2.两个标注者可能都偏好某一类别
        
        3.有些一致性是“碰巧猜对”的
        
        ➡ **准确率不能区分“真实一致”与“随机一致”**
        
        ➡ 这就是为什么需要 Cohen’s Kappa。
        
        - **Cohen Kappa 的核心思想**
        
        Cohen Kappa 的公式本质是：
        
        **Kappa =（观察一致率 − 随机一致率） ÷（1 − 随机一致率）**
        
        换句话说：
        
        > 真实一致性 = 表面一致性 − 偶然一致性
        > 
        
        Kappa 会扣除“猜对的部分”，让我们得到更真实可靠的一致性评价。
        
    
2. **Uniqueness (唯一性)**：数据集中是否存在重复记录。
    - **为什么重要：** 如果训练集和测试集里有重复代码，就像“考试前透题”，模型根本没学会找漏洞，只是背下了答案。这叫***数据泄露 (Data Leakage)***。
    - **怎么测：** 使用代码克隆检测工具（Code Clone Detection），专门找 **Type-3 Clones**（语句稍有修改但逻辑一样的代码）。
        - **1.Type-3 Clones (三型克隆) 定义：** 指的是语句级别的差异。代码片段在语法上相似，但可能添加、修改或删除了某些语句 。
        
        ![image.png](Data%20Quality%20for%20Software%20Vulnerability%20Datasets/image%201.png)
        
        *We followed standard practices and considered type-3 code clones as duplicates [49]. Even functionally similar code fragments will include duplicated patterns and tokens that can adversely affect the model performance and evaluation.However, for software vulnerability datasets, slight functional changes can form the difference between a vulnerable and non-vulnerable label. A typical vulnerability fix only alters a few lines of code [65]. It is important that a model is able to capture these slight functional differences across prediction classes to avoid excessive false positive or false negative rates[24]. Hence, we only considered duplicates with the same labels (vulnerable or non-vulnerable) as code clones.(Page 7)*
        
        - 2.使用工具：Allamanis 检测器
            - 作者复用了 **Allamanis** 开发的代码重复检测工具 。
            - 这是一篇发表在 OOPSLA 2019 上的著名论文（*The adverse effects of code duplication in machine learning models of code*）中提出的工具。
        - 3. 关键配置：调整 Token 阈值 (The Secret Sauce)
            
            这是最细节的一步。作者对工具参数进行了微调：
            
            - **参数调整：** 将样本的 **最小 Token 数量 (minimum token count) 降低到了 5** 。
            - **原因：** 原版工具是设计用来检测整个**文件 (Files)** 的重复，文件通常很大。但这篇论文研究的是**函数 (Functions)**，函数通常很短小。如果不把阈值调低，那些短小的函数就会被漏掉，检测就不准确了 。
        - 4. 附加约束：必须“同类”才算重复
            
            作者加了一个非常重要的限制条件：**只有当两个重复代码的标签（Label）也相同时，才算作 Uniqueness 问题** 。
            
            - **如果代码一样，但标签不一样（一个说有漏洞，一个说没漏洞）：** 这属于 **Consistency（一致性）** 问题，而不是唯一性问题 。作者把这两类问题分得很清。
        - 5. 计算得分
            
            工具会输出一堆“重复群组 (Clusters of duplicates)” 。
            
            - **最终得分：** 计算数据集中“非重复样本”的比例。得分在 **0 到 1** 之间，**1 代表完全没有重复（质量最好）** 。
        
3. **Consistency (一致性)**：数据是否存在矛盾（例如，相同的代码片段被标记为不同的类别）。
    - **定义：** 同样的代码，不能一会儿标记为“有毒”，一会儿标记为“安全” 。
    - **怎么测：** 找完全一样的代码***（Type-1 Clones）***，看它们的标签是否冲突。
        - *为什么要这么严？* 因为代码稍微改一点点（比如加一个 `if` 判断），可能漏洞就被修好了。所以“相似代码”有不同标签是正常的，但“完全相同的代码”有不同标签，那就是绝对的**逻辑矛盾**。
    - **寻找矛盾点：**检测器会在数据集中扫描那些**内容完全相同**，但是**标签不同**（一个标记为 Vulnerable，一个标记为 Non-vulnerable）的样本 。
    - **人工排查原因：**作者手动检查了这些冲突样本，发现原因千奇百怪。例如在 **Big-Vul** 数据集中，因为有些漏洞是“潜伏”的（Latent Vulnerabilities），同一个函数在早期的 Commit 里没被发现，被标记为“无毒”；后来被发现了，又被标记为“有毒”。这种时间差导致了标签冲突 。
    
    ![image.png](Data%20Quality%20for%20Software%20Vulnerability%20Datasets/image%202.png)
    
    **核心逻辑：**
    这张图展示了一个**漏洞引入**到**漏洞修复**的时间轴，揭示了现有的自动标注规则存在“默认安全”的谬误。
    
    **具体流程：**
    
    1. **漏洞引入（Commit 2）：** 程序员无意中在 `func b()` 引入了一个 Bug，但当时没人发现。
    2. **错误标注（Commit 3）：** 后来有一次提交是修复 `func a()` 的。因为 `func b()` 在这次提交中没被修改，数据集的采集脚本就**默认 `func b()` 是安全的（Non-vulnerable）**。但实际上，此时 `func b()` 已经带着那个“潜伏漏洞”了。
    3. **真相大白（Commit i+1）：** 直到很久之后，`func b()` 的漏洞被发现并修复，这时它才被标记为**有漏洞**。
    
    **结论：**这导致了**逻辑冲突**：同样是带着 Bug 的 `func b()` 代码，在 **Commit 3** 被标记为“安全”，而在后续被标记为“有漏洞”。这种因为**发现滞后**导致的数据集噪音，就是**标签不一致性（Inconsistency）**的主要来源 。
    
- **Type-1 Clone 是什么**
    
    **代码内容完全一样，只是空格、换行、缩进或注释不同。**
    

1. **Completeness (完整性)**：数据记录是否包含所有预期的属性值。
    - **核心问题：** 代码是不是被截断了？（比如爬虫没爬全，或者解析脚本写崩了）。
    - **怎么测：** 用语法分析器（Parser）去跑，看函数是不是完整的，有没有被截断 。
        - **自动化语法体检 (Syntactic Parsing)：**作者没有让人去读，而是写了一个自动化的脚本，利用 **C/C++ 的语法分析器 (Parser)** 去跑所有代码 。
        - **判定标准：**
            - **头部缺失 (Start Truncation)：** 比如函数的返回类型（Return Type）丢了。很多老旧的解析器（Parser）比较笨，它们通过搜索函数名（比如 `my_function`）来抓取代码。如果程序员把返回类型 `int` 写在了上一行（C语言中很常见），解析器就会漏抓上一行 。
            - **尾部缺失 (End Truncation)：** 代码逻辑没写完就断了（比如少了大括号 `}`），导致无法编译或解析 。
            - **空值或声明 (Empty/Declaration)：** 尤其是在 **D2A** 数据集中，作者发现了很多“空函数”或者只有一行声明（比如 `void func();`），这种代码里没有任何逻辑，对模型训练毫无意义 。
                - **Empty (空值):**
                    - **定义：** 数据库里有这个样本的 ID，但对应的“代码字段”是空的，或者全是空格。
                    - **来源：** 静态分析工具报错在文件头或者宏定义里，提取脚本找不到对应的函数体，就抓了个空 。
                    
                    - **Declaration(仅声明):**
                        - **定义：** 只有函数的“定义原型”，没有“函数体”（即没有大括号 `{...}` 和里面的逻辑）。
                        - 代码示例：
                        
                        ```jsx
                        c
                        void scan_network_connection();  <-- 这是一个声明，通常在 .h 文件里
                        ```
                        ```
                        
                        - 对比完整函数：
                        
                        ```jsx
                        c
                        void scan_network_connection() {  <-- 这才是我们要的
                            connect(ip);                  <-- 具体的漏洞逻辑在这里
                            ...
                        }
                        ```
                        ```
                        
                        - 后果： 只有声明（Declaration）对模型来说就是废话。模型要学的是“怎么写代码会导致漏洞”，而不是“这个函数叫什么名字”。
            
            *We found truncation at the start of functions to occur predominantly in the Big-Vul dataset. Return types of function definitions were truncated when they were defined over multiple lines, as function parsers commonly start on the line containing the function name. We also found a few functions in the Big-Vul and Devign dataset to be cut off prematurely,
            missing functional lines of code. We were unable to determine the exact cause for this truncation as we did not have access to the scripts used to produce the datasets. We hypothesise that complexities within the source code confuse the lexicographical parser being used to extract them. For instance, many of the early truncated samples contained additional curly brackets (}) within literals.*
            
            *D2A was resilient to truncation but it contains empty missing values, for which no code was provided. These occurred when the static analysis tools flagged lines in a code file outside of any containing function. Furthermore, D2A contains 13,300 single line function declarations that do not contain any functional source code.(Page 9*
            

1. **Currentness (时效性)**：数据是否即时且未过时。
    - **定义：** 数据是不是太老了？。
    - **怎么测：** 这是一个统计学概念，叫 **Concept Drift (概念漂移)**。作者计算了旧数据和新数据在统计分布上的距离（Jensen-Shannon divergence）。
    - **检测方法详解：**
    这部分用到了比较高阶的统计学方法，专门用来检测 ***Concept Drift (概念漂移)***（数据的分布或特征随着时间发生变化，使得过去训练的模型在未来数据上表现变差）。
        - **第一步：新旧切分**作者简单粗暴地把数据集按时间排序，劈成两半：**“最老的一半” (Original)** 和 **“最新的一半” (Current)** 。
        - **第二步：向量化表示 (Bag-Of-Tokens)**把代码切成 Token（词），统计每个 Token 出现的频率。这样就把一堆代码变成了一个**概率分布图** 。
        - **第三步：计算距离 (Jensen-Shannon Divergence)**使用 **JS 散度** 来衡量“老代码分布”和“新代码分布”之间的距离 。
            - *逻辑：* 如果距离很大，说明代码风格或漏洞模式变了（发生了漂移）。如果距离很小，说明这个数据集的时间跨度内，数据特征很稳定。
        - **第四步：验证实验**为了确认这一点，作者还做了一个验证实验：用最早的数据训练模型，然后拿去预测未来不同时间段的数据 。结果发现性能并没有显著下降，说明目前的漏洞数据集在时间上还算稳定，没有严重的过时问题 。
        
        ![image.png](Data%20Quality%20for%20Software%20Vulnerability%20Datasets/image%203.png)
        
        把时间轴切成了 **10 份**，具体操作如下：
        
        - **第一步：按时间排序**先把所有数据样本按**漏洞报告的时间**先后顺序排列 。
        - **第二步：10 等分 (Timestep 0-9)**把排好序的数据平均切成 10 个部分（Partitions）。
        - **第三步：训练集 (Training) —— 代表“过去”**
            - 选取**最老的前 4 份**（Timestep 0, 1, 2, 3，图中绿色部分）作为训练集 。
            - 这模拟了我们在“以前”训练好了模型。
        - **第四步：验证集 (Validation) —— 用于调优**
            - 选取**第 5 份**（Timestep 4，图中深绿色部分）作为验证集，用来调整模型参数 。
        - **第五步：测试集 (Test 1-5) —— 代表“未来”**
            - 剩下的**后 5 份**（Timestep 5, 6, 7, 8, 9，图中五颜六色的部分）被分别当作**独立的测试集** 。
            - **Test 1** 是“近期未来”，**Test 5** 是“远期未来”。
        
        3. 实验逻辑：看跌幅
        
        - **假设：** 如果数据有“保质期”（即存在 Currentness 问题），那么模型在 **Test 1** 上的表现应该最好，在 **Test 5** 上的表现应该**最差**（因为时间隔得最久，漏洞形态可能变了）。
        - **判定标准：** 观察模型性能（如 MCC 指标）是否随着时间推移（从 Test 1 到 Test 5）出现显著下降 。
        
        *We used a similar experimental setup to McIntosh et al. [69] to determine whether vulnerability data is a moving target. We sorted all entries by date and then split each dataset into ten equal partitions. The four earliest partitions were used to train an SVP model, the fifth partition was used for tuning, and the remaining five were used as individual test sets. Figure 5 displays the experiment setup. However, using a Kendall rank correlation test [71] (p > 0.05), we observed no significant decrease in model performance for MCC, precision, or recall as the time between the training and test set increased.(Page 10)我们采用了McIntosh 等人 [69] 类似的实验设置，以确定漏洞数据是否呈现随时间变化的特性。我们首先按日期对所有条目进行排序，然后将每个数据集划分为十个相等的分区。最早的四个分区用于训练 SVP 模型，第五个分区用于调参，其余五个分区则作为独立的测试集。图 5 展示了实验设置。然而，使用Kendall 等级相关检验 [71]（p > 0.05）后，我们并未观察到随着训练集与测试集之间的时间间隔增加，模型在 MCC、精度或召回率方面出现显著的性能下降。*
        

- **研究对象 (Datasets)**：选择了 4 个代表性的 SOTA 漏洞数据集，涵盖了不同的标签来源：
    - **Big-Vul** (由安全供应商提供，来源于 CVE)。
    - **Devign** (由开发者提供，来源于提交历史)。
    - **D2A** (由工具生成，来源于静态分析工具)。
    - **Juliet** (合成数据，人工构建的漏洞模式)。
    
- **实验设计**：
    - **RQ1 (质量评估)**：通过人工审查（针对准确性）、代码克隆检测（针对唯一性）、逻辑检查等方法**量化**各属性指标。
    - **RQ2 (影响分析)**：使用 LineVul（SOTA 模型）进行实验。通过**移除噪声数据或修正标签**后重新训练/测试模型，对比性能变化（Precision, Recall, MCC）。
    
- **Precision（精确率）**
精确率 = 我预测为“有漏洞”的代码里，有多少是真的有漏洞？
    
    ![image.png](Data%20Quality%20for%20Software%20Vulnerability%20Datasets/image%204.png)
    
    - **TP（True Positive）** = 预测有漏洞 & 真的有漏洞
    - **FP（False Positive）** = 预测有漏洞 & 实际没漏洞（误报）
        
        精确率高 → 说明误报少（误报问题）
        
- **Recall（召回率）**
召回率 = 所有真正“有漏洞”的代码里，我找出了多少？
    
    ![image.png](Data%20Quality%20for%20Software%20Vulnerability%20Datasets/image%205.png)
    
    - **FN（False Negative）** = 实际有漏洞 & 却被模型漏掉（漏报）
    召回率高 → 说明漏掉的漏洞少（漏报问题）
- **MCC（马修斯相关系数）**
MCC 是一个更全面、更严格的指标，用于衡量分类模型整体表现。
    
    
    ![image.png](Data%20Quality%20for%20Software%20Vulnerability%20Datasets/image%206.png)
    
    **MCC 的特点：**
    
    - 考虑 **所有四种**情况（TP、TN、FP、FN）
    - 特别适合 **类别极度不平衡的任务**
    - 比 F1、准确率更公平
    - 漏报 + 误报都会影响它
    - 常用于学术研究
    
    ![image.png](Data%20Quality%20for%20Software%20Vulnerability%20Datasets/image%207.png)
    

### 3. Research Questions (研究问题)

- **RQ1**: 当前最先进的软件漏洞数据集中存在哪些数据质量问题？ (What data quality issues are present in the state-of-the-art software vulnerability datasets?)
    - 作者不想空谈数据质量差，而是希望利用 ISO/IEC 25012 标准中的 5 个维度（如准确性、唯一性），对 4 个主流 SOTA 数据集进行一次全面的‘体检’，把模糊的质量问题变成具体的数字（比如 70% 的标签错误）。
- **RQ2**: 这些数据质量问题在多大程度上影响下游的软件漏洞预测模型？ (To what extent do data quality issues impact downstream software vulnerability prediction models?)
    - 作者通过对比实验（Ablation Study），定量地计算了当数据质量差时，模型性能（Precision, Recall, MCC）会下降多少。这证明了数据清洗不仅仅是锦上添花，而是决定模型生死的关键因素。
    
    ![image.png](Data%20Quality%20for%20Software%20Vulnerability%20Datasets/image%208.png)
    
- 中间的方框写着 **"Analysis - RQ1 Measure Attributes"** (分析与测量)。
- 下面的方框写着 **"Impact - RQ2 Validate Attribute Impact"** (影响与验证)。
- 箭头从 RQ1 指向 RQ2，直观地证明了层层递进的逻辑关系。

### 4. Results (研究结果)

- **总体发现**：所有分析的数据集都存在不同程度的数据质量问题。

![image.png](Data%20Quality%20for%20Software%20Vulnerability%20Datasets/image%209.png)

- **Accuracy (准确性)**：
    - **问题**：真实世界数据集（Big-Vul, Devign, D2A）中，**20-71%** 的漏洞标签是不准确的。主要原因包括无关的代码变更、清理代码混入以及修复识别错误。
    - **影响**：修正标签错误后，模型的 **Precision 下降了 29-80%**。这表明模型在原始数据上学习到了错误的模式（将非漏洞代码误判为漏洞）。
- **Uniqueness (唯一性)**：
    - **问题**：数据集中存在严重的重复问题，重复率在 **17-99%** 之间。特别是 D2A 数据集，由于其生成机制，导致了极高的重复率。
    - **影响**：数据重复导致训练集和测试集之间存在数据泄露（Data Leakage），从而人为地夸大了基准测试的性能。去除重复后，D2A 的评估性能 (MCC) **下降了 82%**。
- **Consistency (一致性)**：
    - **问题**：D2A 数据集的一致性较差 (0.531)，存在大量相同的代码片段被标记为相反类别的情况。
    - **影响**：消除不一致性后，模型性能显著提升，特别是在 D2A 和 Juliet 数据集上，因为模型不再被矛盾的标签所困扰。
    
    ![image.png](Data%20Quality%20for%20Software%20Vulnerability%20Datasets/image%2010.png)
    

          **这张表证明了数据的一致性是模型训练的基础门槛。** 最典型的例子是 **D2A 数据集（第三行）**：在清洗前，因为严重的一致性问题，模型无法收敛，指标均为 0。但一旦移除冲突样本（最右列），模型性能直接恢复到 SOTA 水平（MCC 0.748）。当标签冲突过于严重时，模型会完全无法收敛；而消除这种不一致性，能显著消除模型的困惑，大幅提升预测的精确度。

- **Completeness & Currentness**：
    - **结论**：这两个属性在所选数据集中表现相对较好，虽然存在少量截断或缺失，但对模型性能的负面影响不具有统计显著性。
    
    ![image.png](Data%20Quality%20for%20Software%20Vulnerability%20Datasets/image%2011.png)
    
    1. **现象：** Big-Vul 有惊人的 **32,973** 个样本出现了 **Start Truncation**（头部截断）。
    - **原因解析（论文原理解释）：**
        - 这是数据提取脚本的问题。C/C++ 函数定义有时候会写成多行（比如返回类型单独一行），普通的解析器（Lexicographical parser）往往只从函数名那一行开始抓取，结果把上一行的返回类型给漏掉了 。
        - *对模型影响：* 虽然不仅是致命伤，但会破坏代码的语法结构，让基于 AST（抽象语法树）的模型报错。
    1. **现象：** D2A 没有截断问题（全是 0），但它有 **10,824 个 Empty** 和 **13,300 个 Declaration**。
    - **原因解析：**
        - D2A 是由**静态分析工具**生成的。
        - **Empty：** 工具报错的位置可能在函数外面，导致提取不到对应的函数代码 。
        - **Declaration：** 工具扫描到了头文件里的函数声明。
        - *对模型影响：* **这是致命的！** “Declaration”里面没有任何逻辑代码（没有 `if`, `while`, 变量赋值等），拿这种“空壳”去训练模型找漏洞，模型根本学不到任何东西。这就是纯粹的噪音。
    
    ---
    
    ![image.png](Data%20Quality%20for%20Software%20Vulnerability%20Datasets/image%2012.png)
    

这张图展示了自动化清洗后的数据留存率，它揭露了 D2A 等合成/工具生成数据集存在极高比例的重复噪音，也反映了当前漏洞检测领域面临的**数据质量 vs 数据规模**的严峻权衡。