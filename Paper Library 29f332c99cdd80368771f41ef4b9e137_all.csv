Title,Authors,Category,Link,My Takeaway,Rating,Status,Topic / Keywords,Venue,Year
Revisit Self-Debugging with Self-Generated Tests for Code Generation,Xiancai Chen; Zhengwei Tao; Kechi Zhang; Changzhi Zhou; Xinyu Zhang; Wanli Gu; Yuanpeng He; Mengdi Zhang; Xunliang Cai; Haiyan Zhao; Zhi Jin,Method,https://aclanthology.org/2025.acl-long.881/,研究自我调试范式在代码生成中的效果，比较 post-execution 与 in-execution 两种机制。发现前者易受自生成测试偏差影响，而 in-execution 通过在执行中监控变量状态与中间反馈，有效减轻误导并提升鲁棒性与准确性，为无测试场景下的 LLM 自我调试提供了新方向。,,To Read,"self-debugging, self-generated tests, code generation, runtime feedback",ACL 2025 (Long),2025
MGDebugger: Closing the Last Mile of Code Generation with Hierarchical Debugging,Yuling Shi; Songsong Wang; Chengcheng Wan; Xiaodong Gu,Method,https://openreview.net/forum?id=dwQIVcW1du,"提出 MGDebugger 框架，将代码按层次拆分，从底层语法到高层算法逐步定位并修复错误，大幅提高 LLM 代码生成的正确率。




",⭐⭐⭐⭐（结构清晰，适合学习 LLM 调试思路）,To Read,"hierarchical debugging, code generation, LLM agents",ICLR 2025 (OpenReview),2025
REDO: Execution‑Free Runtime Error Detection for Coding Agents,Shengwei Li et al.,System,https://openreview.net/forum?id=THkF3VWSNv,"提出 REDO 框架，将 LLM 的语义理解与静态分析结合，实现“无需执行”的运行时错误检测。通过新基准 SWED-E 系统评估仓库级任务中错误识别能力，显著提升准确率（+11%）和加权 F1（+9%）。该方法为代码智能体提供执行前的高效安全检测思路，减少调试开销并增强稳定性。
",,To Read,"static analysis, runtime error detection, coding agents",ICLR 2025 (OpenReview),2025
Improve Code Generation with Feedback,Z. Xu et al.,Method,https://openreview.net/pdf?id=CscKx97jBi,"提出基于反馈的代码生成框架，让 LLM 在生成过程中接收并利用执行反馈（测试结果、正确性信号等），模拟人类调试。通过持续修正机制显著提升 HumanEval、MBPP 等基准性能，展示 LLM 在“自我调试式”代码生成中的潜力。
",,To Read,"self-debugging pipeline, execution feedback",ICLR 2025 (OpenReview),2025
RepoST: Scalable Repository‑Level Coding Environment Construction with Sandbox Testing,,System / Dataset,https://icml.cc/virtual/2025/48167,"提出了 RepoST 框架，用于在代码仓库级别自动构建可执行的代码生成与测试环境，通过沙盒测试（sandbox testing）来隔离函数依赖、提供执行反馈，从而在不牺牲可扩展性的情况下支持大规模训练与评估。
该方法能自动从真实代码库中提取任务环境并生成训练数据集（RepoST-Train），显著提升 LLM 的代码生成性能（HumanEval 上提升 5.5%，RepoEval 上提升 3.5%）。
",,To Read,"sandbox testing, execution feedback, dataset construction",ICML 2025 (Workshop: CODEML),2025
LLM-Powered Test Case Generation for Detecting Bugs in Plausible Programs (TrickCatcher),Kaibo Liu; Zhenpeng Chen; Yiyang Liu; Jie M. Zhang; Mark Harman; Yudong Han; Yun Ma; Yihong Dong; Ge Li; Gang Huang,Benchmark / Tool,https://aclanthology.org/2025.acl-long.20/,提出 TrickCatcher：用LLM生成程序变体与输入，运行对比输出以抓“假阳性”代码中的隐藏bug。,,To Read,test case generation; bug detection; plausible programs; execution,ACL 2025 (Long),2025
Dynamic Scaling of Unit Tests for Code Reward Modeling,Zhiyuan Ma; Shizhe Diao; Yong Lin; Wendi Ji; Xipeng Qiu,Method,https://aclanthology.org/2025.acl-long.343/,根据题目难度动态扩增/收缩单元测试数量，提升代码奖励模型的稳定性与效果。,,To Read,unit tests; reward modeling; code generation; execution-based eval,ACL 2025 (Long),2025
DebateCoder: Towards Collective Intelligence of LLMs via Test Case-Driven Debate for Code Generation,Jingwei Chen; Songbo Tan; Ningyu Zhang; Shumin Deng; Huajun Chen,Method,https://aclanthology.org/2025.acl-long.589/,两模型用测试样例相互‘攻击—防御’，通过对比分析与执行结果不断修复代码。,,To Read,test-driven debate; execution feedback; code repair,ACL 2025 (Long),2025
FEA-Bench: A Benchmark for Evaluating Repository-Level Feature Implementation of LLMs,Weili Li; Xin Zhang; Hongfa Wang; et al.,Benchmark,https://aclanthology.org/2025.acl-long.839/,从真实PR构建任务，并配套测试文件，考查LLM在仓库级新增功能上的可通过性。,,To Read,repository-level; feature implementation; unit tests; execution-based scoring,ACL 2025 (Long),2025
Can You Really Trust Code Copilot? Evaluating Large Language Models on Code Security,Yichao Mou; et al.,Benchmark,https://aclanthology.org/2025.acl-long.849/,提出CoV‑Eval与VC‑Judge，系统评估LLM在漏洞检测与修复上的可靠性。,,To Read,security; vulnerability detection/repair; evaluation,ACL 2025 (Long),2025
CodeDPO: Aligning Code Models with Self Generated and Curated Test Cases,Kexun Zhang; et al.,Method,https://aclanthology.org/2025.acl-long.771/,把可执行测试作为偏好信号进行DPO对齐，让代码模型更“通过测试”。,,To Read,preference optimization; test cases; alignment,ACL 2025 (Long),2025
M2RC-EVAL: Massively Multilingual Repository-level Code Completion Benchmark,Jin Liu; et al.,Benchmark,https://aclanthology.org/2025.acl-long.763/,18种语言的仓库级补全评测，细粒度拆分多种场景，附配套指令数据集。,,To Read,repository-level; multilingual; completion; evaluation,ACL 2025 (Long),2025
Automated Real-World Repo-Level Compilation with Tool Agents (AutoCompiler),Linyang Hu; et al.,System,https://aclanthology.org/2025.acl-long.103/,构建编译代理链路，自动完成真实仓库的依赖修复与编译通过。,,To Read,compilation; build & test; tool-augmented agents,ACL 2025 (Long),2025
Can LLMs Reason About Program Semantics? A Benchmark of Formal Reasoning on Code (FormalBench),Thanh Le-Cong; et al.,Benchmark,https://aclanthology.org/2025.acl-long.1068/,用形式化语义任务测试LLM的代码推理与自修复能力。,,To Read,program semantics; formal reasoning; self-repair prompts,ACL 2025 (Long),2025
LONGCODEU: Benchmarking Long-Context Language Models on Long Code Understanding,Jiawei Li; et al.,Benchmark,https://aclanthology.org/2025.acl-long.1324/,从真实仓库抽取长代码，评测函数识别、数据流/跨单元关系等理解能力。,,To Read,long-context; repository code; analysis; evaluation,ACL 2025 (Long),2025
Acing Coder RL via Automated Test-Case Synthesis,Hang Zeng; et al.,Method,https://aclanthology.org/2025.acl-long.587/,大规模自动合成测试样例，解决代码RL缺乏可靠奖励的问题。,,To Read,reinforcement learning; test synthesis; reward,ACL 2025 (Long),2025
Benchmarking LLMs and LLM-based Agents in Practical Vulnerability Detection for Code Repositories,Alperen Yildiz; Sin G Teo; Yiling Lou; Yebo Feng; Chong Wang; Dinil Mon Divakaran,Benchmark,https://aclanthology.org/2025.acl-long.1490/,在真实仓库中评测LLM/代理的漏洞检测与审计能力。 JITVul 构建了首个针对跨函数漏洞检测的系统性评测基准，揭示了 LLM 安全代码分析在精确度与上下文推理上的瓶颈。,,To Read,security; vulnerability detection; repo-level eval,ACL 2025 (Long),2025
ToolCoder: A Systematic Code-Empowered Tool Learning Framework,Hongxu Ding; et al.,System,https://aclanthology.org/2025.acl-long.874/,以代码为核心的工具学习框架，利用报错回溯机制进行系统化调试。,,To Read,tool learning; execution reliability; debugging via traceback,ACL 2025 (Long),2025
Can Language Models Replace Programmers for Coding? REPOCOD Says ‘Not Yet’,Shanchao Liang; Nan Jiang; Yiran Hu; Lin Tan,Benchmark,https://aclanthology.org/2025.acl-long.1204/,"构建了 RepoCod，一个面向真实项目级（repository-level）代码生成的复杂基准集，用于评估 LLM 在真实软件开发任务中的表现。
包含来自 11 个热门项目的 980 个整函数级任务，其中约 50% 需要跨文件依赖，且每个任务有 314 个开发者编写的测试样例。
实验表明，现有 10 个主流 LLM 在 RepoCod 上表现均不理想（pass@1 不超过 30%），揭示了 LLM 在真实依赖上下文中的局限性。
此外，加入 检索增强（retrieval-augmented） 信息能显著提升代码生成质量。",,To Read,repository-level; complex dependencies; evaluation,ACL 2025 (Long),2025
Tree-of-Evolution: Tree-Structured Instruction Evolution for Code Generation in LLMs,Ziyang Luo; Kaixin Li; Hongzhan Lin; Yuchen Tian; Mohan Kankanhalli; Jing Ma,Method,https://aclanthology.org/2025.acl-long.14/,"提出 Tree-of-Evolution (ToE) 框架，用树状结构建模代码指令合成过程（code instruction synthesis），以克服现有方法单向生成与随机性导致的质量与多样性限制。
ToE 通过多路径演化探索与优化驱动的进化策略，在每一步生成时基于前一步的质量进行改进。
在 HumanEval、MBPP、LiveCodeBench、BigCodeBench 等基准上，ToE 仅使用 7.5 万条合成数据即可达到或超越使用数百万样本微调模型（如 Qwen2.5-Coder-Instruct）的性能。",,To Read,code generation; instruction evolution; robustness,ACL 2025 (Long),2025
METAL:A Multi-Agent Framework for Chart Generation with Test-Time Verification,Bohan Li; et al.,System,https://aclanthology.org/2025.acl-long.1452/,提出 METAL 框架，一个基于视觉语言模型（VLM）的多智能体系统，用于自动化图表生成（chart generation）。该方法将复杂的视觉+代码生成任务拆解为多个专门智能体的协作过程，每个智能体负责不同的子任务（如布局、文本、颜色等）。相比直接让模型生成图表代码，METAL 在图表生成任务中提升了 5.2% 的 F1 分数，并在 LLaMA-3.2-11B 上较直接提示提升 11.33%。系统还展现出“测试时扩展性（test-time scaling）”现象，即随着推理计算预算增长，性能单调提升。,,To Read,code generation (charts); test-time verification,ACL 2025 (Long),2025
Vulnerability Detection with Code Language Models: How Far Are We?,Yangruibo Ding; Yanjun Fu; Omniyyah Ibrahim; Chawin Sitawarin; Xinyun Chen; Basel Alomair; David Wagner; Baishakhi Ray; Yizheng Chen,Benchmark,https://arxiv.org/abs/2403.18624,现有代码漏洞检测数据集质量低且存在数据泄露，使模型性能被严重高估；PrimeVul 通过更准确标注、严格去重与时间切分揭示出当前模型在真实场景中几乎失效，说明漏洞检测领域亟需更可靠基准与更强模型。,,Finished,"Vulnerability Detection（漏洞检测）
Dataset Quality（数据集质量）Temporal Split（时间切分）Deduplication（去重）
Benchmark（基准评测）",ICSE 2025,2025
icodeReviewer：Improving Secure Code Review with Mixture of Prompts,,,,,,,,,
Data Quality for Software Vulnerability Datasets,"Roland Croft, M. Ali Babar, M. Mehdi Kholoosi",System / Dataset,https://arxiv.org/abs/2301.05456,"核心发现：研究了四个SOTA漏洞数据集（Big-Vul, Devign, D2A, Juliet），发现它们普遍存在严重的质量问题。具体数据：真实世界数据集的标签错误率高达 20-71%，且存在 17-99% 的数据重复率。影响：这些数据质量问题（准确性、唯一性、一致性）会导致漏洞预测模型（SVP）的性能评估虚高或无法有效学习，强调了数据清洗的重要性。",,Reading,"Software Vulnerability, Data Quality, Machine Learning, Deep Learning",ICSE 2023,2023